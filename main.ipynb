{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import re\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datasets import concatenate_datasets, load_dataset_builder, get_dataset_config_names, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load Social Bias Frames dataset\n",
    "ds_builder = load_dataset_builder(\"social_bias_frames\")\n",
    "print(ds_builder.info.description)\n",
    "configs = get_dataset_config_names(\"social_bias_frames\")\n",
    "print(configs)\n",
    "\n",
    "# train\n",
    "train_data = load_dataset(\"social_bias_frames\", split=\"train\")\n",
    "# test\n",
    "test_data = load_dataset(\"social_bias_frames\", split=\"test\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99b41e3ead5fd2a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if links and emojis in text\n",
    "def has_links(text):\n",
    "    # Define a regular expression for URLs\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    return int(bool(re.search(url_pattern, text)))\n",
    "\n",
    "\n",
    "def has_emojis(text):\n",
    "    # Define a regular expression for emojis\n",
    "    emoji_pattern = r'[\\U00010000-\\U0010ffff]'\n",
    "    return int(bool(re.search(emoji_pattern, text)))\n",
    "\n",
    "\n",
    "# Apply the custom functions to the dataset\n",
    "def apply_custom_features(dataset):\n",
    "    dataset = dataset.map(lambda x: {\"HasLinksYN\": 'Y' if has_links(x['post']) else 'N'})\n",
    "    dataset = dataset.map(lambda x: {\"HasEmojisYN\": 'Y' if has_emojis(x['post']) else 'N'})\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6de395c49b31b9ad"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = apply_custom_features(train_data)\n",
    "test_data = apply_custom_features(test_data)\n",
    "\n",
    "# Separate the features and labels for training and testing data\n",
    "labels_tr = train_data[\"offensiveYN\"]\n",
    "data_tr = train_data.remove_columns(\"offensiveYN\")\n",
    "labels_te = test_data[\"offensiveYN\"]\n",
    "data_te = test_data.remove_columns(\"offensiveYN\")\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "data_tr = data_tr.to_pandas()\n",
    "data_te = data_te.to_pandas()\n",
    "\n",
    "# Step 2: Preprocess the text data using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10, token_pattern=r'[a-zA-Z]+')\n",
    "X_train = tfidf_vectorizer.fit_transform(data_tr['post'])\n",
    "X_test = tfidf_vectorizer.transform(data_te['post'])\n",
    "\n",
    "print(X_train.shape)  # Check the shape of X_train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d1576d8e46e2a92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine text features and additional features\n",
    "data_tr['HasLinksYN'] = data_tr['HasLinksYN'].map({'N': 0, 'Y': 1})\n",
    "data_tr['HasEmojisYN'] = data_tr['HasEmojisYN'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "data_te['HasLinksYN'] = data_te['HasLinksYN'].map({'N': 0, 'Y': 1})\n",
    "data_te['HasEmojisYN'] = data_te['HasEmojisYN'].map({'N': 0, 'Y': 1})\n",
    "\n",
    "# Select only the numeric columns for converting to sparse matrix\n",
    "numeric_columns_tr = ['HasLinksYN', 'HasEmojisYN'] \n",
    "numeric_columns_te = ['HasLinksYN', 'HasEmojisYN']\n",
    "\n",
    "# Convert additional features to sparse matrix\n",
    "data_tr_sparse = csr_matrix(data_tr[numeric_columns_tr].values)\n",
    "data_te_sparse = csr_matrix(data_te[numeric_columns_te].values)\n",
    "\n",
    "# Combine text features and additional features\n",
    "X_combined_train = hstack([X_train, data_tr_sparse], format='csr')\n",
    "X_combined_test = hstack([X_test, data_te_sparse], format='csr')\n",
    "\n",
    "# Step 4: Prepare labels\n",
    "y_train = labels_tr\n",
    "y_test = labels_te\n",
    "\n",
    "# # Define the parameter grid to search for the best 'max_depth'\n",
    "# param_grid = {'max_depth': range(1, 21)}  # Searching from 1 to 20\n",
    "# \n",
    "# # Initialize the decision tree classifier\n",
    "# dtree = DecisionTreeClassifier(random_state=42)\n",
    "#\n",
    "# # Initialize GridSearchCV with cross-validation\n",
    "# grid_search = GridSearchCV(dtree, param_grid, cv=5, scoring='accuracy')\n",
    "#\n",
    "# # Fit GridSearchCV on the training data\n",
    "# grid_search.fit(X_combined_train, y_train)\n",
    "#\n",
    "# # Print the best parameter and the corresponding score from the training (cross-validation)\n",
    "# print(f\"Best Parameter from training: {grid_search.best_params_}\")\n",
    "# print(f\"Best Score from training: {grid_search.best_score_}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "830ca925210a9c4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 5: Train the Decision Tree Classifier\n",
    "clf = DecisionTreeClassifier(random_state=1)\n",
    "clf.fit(X_combined_train, y_train)\n",
    "\n",
    "# Step 6: Make Predictions on the test set\n",
    "y_pred = clf.predict(X_combined_test)\n",
    "\n",
    "# Step 7: Evaluate the Model\n",
    "accuracy_dt = accuracy_score(y_test, y_pred)\n",
    "report_dt = classification_report(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(f\"Decision Tree Results:\")\n",
    "print(f\"Accuracy: {accuracy_dt}\")\n",
    "print(\"\\nDecision Tree Classification Report:\")\n",
    "print(report_dt)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f875dbbdaefca06b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 8: Train the Naive Bayes Classifier\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(X_combined_train, y_train)\n",
    "\n",
    "# Step 9: Make Predictions on the test set with Naive Bayes\n",
    "y_pred_nb = nb_clf.predict(X_combined_test)\n",
    "\n",
    "# Step 10: Evaluate the Naive Bayes Model\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "report_nb = classification_report(y_test, y_pred_nb, zero_division=0)\n",
    "\n",
    "print(\"\\nNaive Bayes Results:\")\n",
    "print(f\"Accuracy: {accuracy_nb}\")\n",
    "print(\"\\nNaive Bayes Classification Report:\")\n",
    "print(report_nb)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "12cc9f9cd5997b9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Standardize the features before feeding them into MLP\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#MLP Hyperparameters\n",
    "MLPlearn = 0.01\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "trunc = svd.fit_transform(X_train_scaled)\n",
    "trunc_test = svd.transform(X_test_scaled)\n",
    "\n",
    "# Train the Multi-layer Perceptron Classifier\n",
    "mlp_clf = MLPClassifier(random_state=1, activation='relu', max_iter=300, learning_rate='adaptive', learning_rate_init=MLPlearn)\n",
    "mlp_clf.fit(trunc, y_train)\n",
    "\n",
    "# Make Predictions on the test set with MLP\n",
    "y_pred_mlp = mlp_clf.predict(trunc_test)\n",
    "\n",
    "# Evaluate the MLP Model\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "report_mlp = classification_report(y_test, y_pred_mlp, zero_division=0)\n",
    "\n",
    "print(\"\\nMulti-layer Perceptron Results\")\n",
    "print(f\"Accuracy: {accuracy_mlp}\")\n",
    "print(\"\\nMulti-layer Perceptron Classification Report:\")\n",
    "print(report_mlp)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e14eb8d0f41fdbc3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train the Logistic Regression Classifier\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions on the test set with Logistic Regression\n",
    "y_pred_lr = log_reg.predict(X_test)\n",
    "\n",
    "# Evaluate the Logistic Regression Model\n",
    "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
    "report_lr = classification_report(y_test, y_pred_lr, zero_division=0)\n",
    "\n",
    "print(\"\\nLogistic Regression Results\")\n",
    "print(f\"Accuracy: {accuracy_lr}\")\n",
    "print(\"\\nLogistic Regression Classification Report:\")\n",
    "print(report_lr)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "967c17003d9fe4de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
